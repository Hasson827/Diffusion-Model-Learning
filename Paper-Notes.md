# 扩散模型的一般概念

## 1. 定义

2015年的论文将扩散模型描述如下：

- The essential idea is to systematically and slowly destroy structure in a data distribution through an iterative **forward diffusion process**. We then learn a **reverse diffusion process** that restores structure in data, yielding a highly flexible and tractable generative model of the data.

- 其关键理念在于通过一个迭代式的**正向扩散流程**，有系统且逐步地破坏数据分布中的结构。随后，我们学习一种**反向扩散流程**，用于恢复数据中的结构，由此生成一个高度灵活且易于处理的关于数据的生成模型。 

<img src="imgs/Basic_Idea.png">

## 2. 前向过程 (Foward Process)

我们从原始图像开始一步一步地向图像中添加越来越多的噪声，如果我们重复此操作足够多次，图像将变成纯噪声。2015年的论文中决定从正态分布中采样噪声，这一点在后续一直没有变化。

**注意：** 在前向过程中，我们不会在每一步都是用相同数量的噪声，而是由一个缩放均值和方差的调度器来调节的，这确保了**随着噪声的添加，方差不会爆炸。**

从一张图片变成纯噪声是相当简单的过程，但是学习相反的过程是极其困难的

## 3. 反向扩散过程 (Reverse Diffusion Process)

神经网络学习怎样逐步从图像中去除噪声，这样我们就可以给模型一个仅由从正态分布中采样得到的噪声图像，让模型逐渐去除噪声，直到我们得到一个清晰的图像。

## 4. 神经网络

2020年的DDPM论文指出神经网络可以预测的三个变量：

1. 每个时间步的噪声平均值。
2. 直接预测原始图像。(X)
3. 直接预测图像中的噪声，然后可以从噪声图像中减去噪声，得到噪声少一点的图像。

大部分人都选择了第三个，因为我们在训练过程中会计算不同（随机选择）时间步的损失函数，不同任务目标计算得到的结果会根据损失值向不同的“隐含权重”收敛，而“预测噪声”这个目标会使权重更倾向于预测得到更低的噪声量，也就是最典型的 $Minimize$ 任务。你可以通过选择更复杂的目标来改变这种“隐形损失权重”，这样你所选择的噪声调度器就能够直接在高噪声量情况下产生更多样本。

# 背后的数学

## 符号统一

-  $x_{t}$ 表示在时间步 $t$ 时的图像，其中 $x_{0}$ 表示原始图像，而最终遵循各向同性高斯分布的最终图像被称为 $x_{T}$
- $q(x_{t}|x_{t-1})$ 对应于前向过程，其输出图像是在输入图像的基础上叠加了一些小噪声。
- $p(x_{t-1}|x_{t})$ 对应于反向扩散过程，它以 $x_{t}$ 为输入，并使用神经网络生成样本 $x_{t-1}$

## 前向过程

$$
q(x_{t}|x_{t-1}) = \mathcal{N}(x_{t},\sqrt{1-\beta_{t}}\,x_{t-1},\beta_{t}I)
$$
其中：

- $\mathcal{N}$ 是正态分布
- $x_{t}$ 是输出
- $\sqrt{1-\beta_{t}}x_{t-1}$ 是平均值，也可以记为 $\mu_{t}$
- $\beta_{t}I$ 是方差，也可以记为 $\sigma_{t}^{2}$
- $\beta$ 是调度器，$\forall \beta \,,\, \beta \in (0,1) $

现在我们已经知道了前向过程中一步的公式，理论上只要将这个步骤重复1000次就可以得到结果，但是有一种更简单的方法，只用了一步就解决了。

# 四篇论文的不同改进之处

# 总结