# 概率论与机器学习

本文件旨在从最基础的概率论公式开始，逐步推导和讲解相关公式，以帮助读者深入理解概率论的核心概念和应用。

## 1. 概率基本规则

### 离散变量加和公式

$$
p(X) = \sum_{Y}p(X,Y)
$$

### 离散变量乘积公式

$$
p(X,Y) = p(Y|X)p(X) = p(X|Y)p(Y)
$$

### 连续变量加和公式

$$
p(x) = \int p(x,y)dy
$$

### 连续变量乘积公式

$$
p(x,y) = p(y|x)p(x) = p(x|y)p(y)
$$

## 2. 期望和方差

### 离散变量期望

$$
E[f] = \sum_{x}p(x)f(x)
$$

### 连续变量期望

$$
E[f] = \int p(x)f(x)dx
$$

### 二元离散变量期望

$$
E_{x}[f|y] = \sum_{x}p(x|y)f(x)
$$

### 方差公式

$$
var[f] = E[f(x)^{2}]-E[f(x)]^{2}
\\
var[x] = E[x^{2}]=E[x]^{2}
$$

### 协方差公式

$$
cov[x,y] = E_{x,y}[(x-E[x])(y-E[y])] = E_{x,y}[xy]-E[x]E[y]
$$

## 3. 贝叶斯定理

$$
p(w|D) = \frac{p(D|w)p(w)}{p(D)}
$$

## 4. 高斯分布

### 基本公式

$$
\mathcal{N}(x|\mu,\sigma^{2}) = \frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{(x-\mu)^2}{2\sigma^{2}}}
$$

### 期望和方差

$$
E[x] = \mu
\\
var[x] = \sigma^{2}
$$

### 高维高斯分布

$$
\mathcal{N}(x|\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{D}{2}}} \frac{1}{|\Sigma|^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^{T}\Sigma^{-1}(x-\mu)}
$$

其中：

- $D$ 是维度
- $\Sigma$ 是一个 $D\times D$ 的协方差矩阵
- $\mu$ 是一个维度为 $D$ 的向量
- $|\Sigma|$ 是 $\Sigma$ 的行列式

### 似然函数

假设 $\mathbf{x}$ 是一个独立同分布的样本集合，似然函数可以表示为：

$$
L(\mu, \sigma^2) = \prod_{i=1}^{n} \mathcal{N}(x_i|\mu,\sigma^{2}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{(x_i-\mu)^2}{2\sigma^{2}}}
$$

### 对数似然函数

在实践中，最大化似然函数的对数更方便。因为对数是其参数的单调递增函数，所以函数对数的最大化等同于函数本身的最大化。取对数不仅简化了后续的数学分析，而且在数值上也有帮助，因为大量小概率的乘积很容易使计算机的数值精度下溢，而这可以通过计算对数概率的总和来解决。

$$
\log L(\mu, \sigma^2) = \log \left( \prod_{i=1}^{n} \mathcal{N}(x_i|\mu,\sigma^{2}) \right) = \sum_{i=1}^{n} \log \left( \frac{1}{\sqrt{2\pi \sigma^{2}}}e^{-\frac{(x_i-\mu)^2}{2\sigma^{2}}} \right)
$$

进一步简化得到：

$$
\log L(\mu, \sigma^2) = -\frac{N}{2} \log (2\pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{n} (x_i - \mu)^2
$$

将该函数视为 $\mu$ 的函数并求最大值，可得此时 $\mu$ 的取值为：

$$
\mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n}
$$

- 这是 $\mathbf{x}$ (作为样本集合)的均值

将该函数视为 $\sigma$ 的函数并求最大值，可得此时 $\sigma$ 的取值为：

$$
\sigma_{ML}^{2} = \frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}
$$

- 这是 $\mathbf{x}$ (作为样本集合)的方差

考虑 $\mu_{ML}$ 和 $\sigma_{ML}^{2}$ 的期望：

$$
E[\mu_{ML}] = \mu
\\
E[\sigma_{ML}^{2}] = \left( \frac{N-1}{N} \right)\sigma^{2}
$$

> **证明：**
>
> $$
> E[\sigma_{ML}^{2}] = E\left[\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}\right]
> $$
>
> 由于 $\mu_{ML}$ 是样本均值，可以写成：
>
> $$
> E[\sigma_{ML}^{2}] = E\left[\frac{1}{N}\sum_{n=1}^{N}\left((x_{n}-\mu) - (\mu_{ML}-\mu)\right)^{2}\right]
> $$
>
> 展开平方项：
>
> $$
> E[\sigma_{ML}^{2}] = E\left[\frac{1}{N}\sum_{n=1}^{N}\left((x_{n}-\mu)^{2} - 2(x_{n}-\mu)(\mu_{ML}-\mu) + (\mu_{ML}-\mu)^{2}\right)\right]
> $$
>
> 由于 $E[x_{n}-\mu] = 0$ 和 $E[(x_{n}-\mu)(\mu_{ML}-\mu)] = 0$，可以简化为：
>
> $$
> E[\sigma_{ML}^{2}] = E\left[\frac{1}{N}\sum_{n=1}^{N}(x_{n}-\mu)^{2}\right] - E\left[(\mu_{ML}-\mu)^{2}\right]
> $$
>
> 注意到 $E[(\mu_{ML}-\mu)^{2}] = \frac{\sigma^{2}}{N}$，所以：
>
> $$
> E[\sigma_{ML}^{2}] = \sigma^{2} - \frac{\sigma^{2}}{N} = \left(\frac{N-1}{N}\right)\sigma^{2}
> $$

因此将方差乘以一个系数

$$
\tilde \sigma^{2} = \frac{N}{N-1}\sigma_{ML}^{2} = \frac{1}{N-1}\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}
$$

## 5. 从概率角度看曲线拟合

曲线拟合问题的目标是能够根据一组训练数据，给定输入变量 $x$ 的某个新值，对目标变量 $$t$ 进行预测，这些数据包含 $N$ 个输入值 $x = （x_{1},···,x_{N}) ^{T}$ 及其相应的目标值 $t = （t_{1},···,t_{N})^{T}$。我们可以使用概率分布来表示我们对目标变量值的不确定性。为此，我们将假设，给定 x 的值，相应的 t 值具有高斯分布，其平均值等于 $y(x,w) = \sum_{j=0}^{N}w_{j}x^{j}$ 给出的多项式曲线的 $y(x,w)$ 的值。因此我们有：

$$
p(t|x,w,\beta) = \mathcal{N}(t|y(x,w),\beta^{-1})
$$

其中 $\beta = \frac{1}{\sigma^{2}}$ 被称为精度

则我们可以得到其似然函数：

$$
p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta) = \prod_{n=1}^{N}\mathcal{N}(t_{n}|y(x_{n}\mathbf{w}),\beta^{-1})
$$

将其变为对数似然函数：

$$
\log p(\mathbf{t}|\mathbf{x},\mathbf{w},\beta) = \sum_{n=1}^{N} \log \mathcal{N}(t_{n}|y(x_{n},\mathbf{w}),\beta^{-1}) = -\frac{N}{2} \log (2\pi) + \frac{N}{2} \log \beta - \frac{\beta}{2} \sum_{n=1}^{N} (t_{n} - y(x_{n},\mathbf{w}))^2
$$

首先考虑确定多项式系数的最大似然解，该系数将用 $w_{ML}$ 表示。这些是通过以 $w$ 为自变量而将对数似然函数最大化来确定的。为此，我们可以省略上式右侧的前两项，因为它们与 $w$ 无关。此外，我们还注意到，用正常数系数缩放对数似然不会改变 $w$ 在函数最大时的取值，因此我们可以将系数 $\frac{β}{2}$ 替换为 $\frac{1}{2}$ 。最后，我们可以等效地最小化负对数似然，而不是最大化对数似然。因此，我们看到，就确定$w$而言，最大化似然等同于最小化平方和误差函数。因此，平方和误差函数是在高斯噪声分布的假设下最大化似然的结果。

对于 $\beta$ 而言，我们同样可以通过最小化下式来得到其最大值： 

$$
\frac{1}{\beta_{ML}} = \frac{1}{N}\sum_{n=1}^{N}y((x_{n},\mathbf{w}_{ML})-t_{n})^{2}
$$

确定了参数 $\mathbf{w}$ 和 $\beta$ 之后，我们现在就可以预测 $x$ 的新值了。因为我们现在有一个概率模型，这个模型是用预测分布来表示的，该分布给出了 $t$ 上的概率分布，而不是简单的点估计。

将前面求得的参数代回似然函数可得：

$$
p(\mathbf{t}|\mathbf{x},\mathbf{w_{ML}},\beta_{ML}) = \prod_{n=1}^{N}\mathcal{N}(t_{n}|y(x_{n}\mathbf{w_{ML}}),\beta_{ML}^{-1})
$$

在确定了模型参数后，我们可以对新的输入数据进行预测。预测分布可以表示为：

$$
p(t|x, \mathbf{w_{ML}}, \beta_{ML}) = \mathcal{N}(t|y(x, \mathbf{w_{ML}}), \beta_{ML}^{^{-1}})
$$

其中， $y(x,\mathbf{w_{ML}})$ 是基于最大似然估计的多项式曲线， $\beta_{ML}^{-1}$ 是噪声的方差。

通过这个预测分布，我们不仅可以得到预测值的期望，还可以得到预测值的不确定性。

现在让我们朝着更贝叶斯的方法迈出一步，并引入多项式系数 $w$ 的先验分布。为简单起见，我们考虑以下形式的高斯分布：

$$
p(\mathbf{w}|\alpha) = \mathcal{N}(\mathbf{w}|0,\alpha^{-1}\mathbf{I}) = \left( \frac{\alpha}{2\pi} \right)^{\frac{M+1}{2}}e^{-\frac{\alpha}{2}\mathbf{w}^{T}\mathbf{w}}
$$

其中：

- $\alpha$ 是分布的精度，是超参数。
- $M+1$ 是 $M$ 阶多项式的向量 $\mathbf{w}$ 中的元素总数。

贝叶斯定理给出了后验分布的表达式：

$$
p(\mathbf{w} \mid \mathbf{x}, \mathbf{t}, \alpha, \beta) = \frac{p(\mathbf{t} \mid \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha)}{p(\mathbf{t} \mid \mathbf{x}, \alpha, \beta)}
$$

其中：

- $p(\mathbf{w} \mid \mathbf{x}, \mathbf{t}, \alpha, \beta)$ 是参数 $\mathbf{w}$ 的后验分布，条件是数据 $\mathbf{x}$ 和 $\mathbf{t}$ ，以及超参数 $\alpha$ （先验的精度）和 $\beta$ （似然的精度）。
- $p(\mathbf{t} \mid \mathbf{x}, \mathbf{w}, \beta)$ 是似然函数，表示给定参数 $\mathbf{w}$ 和数据 $\mathbf{x}$ ，目标变量 $\mathbf{t}$ 的概率分布。
- $p(\mathbf{w} \mid \alpha)$ 是参数 $\mathbf{w}$ 的先验分布，通常由超参数 $\alpha$ 控制。
- $p(\mathbf{t} \mid \mathbf{x}, \alpha, \beta)$ 是证据（evidence）或边缘似然（marginal likelihood），它是 $\mathbf{w}$ 上的积分：

$$
p(\mathbf{t} \mid \mathbf{x}, \alpha, \beta) = \int p(\mathbf{t} \mid \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha) \, d\mathbf{w}
$$

由于证据 $p(\mathbf{t} \mid \mathbf{x}, \alpha, \beta)$ 是一个归一化常数（与 $\mathbf{w}$ 无关），在计算后验分布的形状时可以省略。因此，后验分布与以下表达式的乘积成正比：

$$
p(\mathbf{w} \mid \mathbf{x}, \mathbf{t}, \alpha, \beta) \propto p(\mathbf{t} \mid \mathbf{x}, \mathbf{w}, \beta) p(\mathbf{w} \mid \alpha)
$$

我们现在可以通过找到给定数据的最可能 $\mathbf{w}$ 值来确定 $\mathbf{w}$ ，换句话说，通过最大化后验分布来确定 $\mathbf{w}$ 。这种技术称为最大后验，或简称为 MAP。我们将似然函数都取负对数后把与 $\mathbf{w}$ 无关的项全部丢掉，最后得到优化目标，即最小化下式：

$$
\frac{\beta}{2} \sum_{n=1}^N \{y(x_n, \mathbf{w}) - t_n\}^2 + \frac{\alpha}{2} \mathbf{w}^T \mathbf{w}
$$

在曲线拟合问题中，我们得到训练数据 $\mathbf{x}$ 和 $\mathbf{t}$ ，以及一个新的测试点 $x$ ，我们的目标是预测 $t$ 的值。因此，我们希望评估预测分布 $p(t|x,\mathbf{x},\mathbf{t}）$ 。在这里，我们将假设参数 $\alpha$ 和 $\beta$ 是固定的，并且事先已知。

应用乘积公式可得：

$$
p(t|x,\mathbf{x},\mathbf{w}) = \int p(t|x,\mathbf{w})p(\mathbf{w|x,t})d\mathbf{w}
$$

为了符号简便，此处将所有的 $\alpha$ 和 $\beta$ 都删去了。最终结果是预测分布可以写成如下形式：

$$
p(t \mid x, \mathbf{x}, \mathbf{t}) = \mathcal{N}(t \mid m(x), s^2(x))
$$

- 其中均值和方差分别由以下公式给出：

$$
m(x) = \beta \phi(x)^T \mathbf{S} \sum_{n=1}^N \phi(x_n) t_n
$$

$$
s^2(x) = \beta^{-1} + \phi(x)^T \mathbf{S} \phi(x)
$$

- 矩阵 $\mathbf{S}$ 由以下表达式给出：

$$
\mathbf{S}^{-1} = \alpha \mathbf{I} + \beta \sum_{n=1}^N \phi(x_n) \phi(x_n)^T
$$

- 其中 $\mathbf{I}$ 是单位矩阵
- 向量 $\phi(x)$ 的元素为 $\phi_i(x) = x^i$，其中 $i = 0, \dots, M$。