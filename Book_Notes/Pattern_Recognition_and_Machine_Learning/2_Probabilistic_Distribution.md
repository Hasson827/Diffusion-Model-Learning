# Probabilistic Distribution

## 二进制变量（0-1变量）

考虑一个二进制随机变量 $x \in \left\{ 0,1 \right\}$ ，其取1的概率为 $\mu \in [0,1]$ ，则我们能够给出下式：

$$
\begin{aligned}
p(x=1\,|\,\mu) &= \mu \\
p(x=0\,|\,\mu) &= 1-\mu
\end{aligned}
$$

即在给定 $\mu$ ，也就是已知 $\mu$ 的情况下，我们能够知道 $p(x=1)=\mu$ 且 $p(x=0)=1-\mu$ ，这是十分简单且显然的事情。

于是我们能够将上述两式统一得到**伯努利分布(Bernoulli Distribution)**

$$
Bern(x|\mu) = \mu^{x}(1-\mu)^{1-x}
$$

并且十分容易可证明它是归一化的，且其均值和方差由下式给出：

$$
\begin{aligned}
E[x] &= \mu
\\
var[x] &= \mu (1-\mu)
\end{aligned}
$$

现在假设我们有一个数据集 $\mathcal{D} = \left\{ x_{1},\dots,x_{N} \right\}$ 并且每个样本之间相互独立，那么我们可以得到其似然函数

$$
p(\mathcal{D} | \mu) = \prod_{n=1}^{N}p(x_{n}|\mu) = \prod_{n=1}^{N}\mu^{x_{n}}(1-\mu)^{x_{n}}
$$

我们要将似然函数最大化，等价于将对数似然函数最大化，因此我们需要最大化以下函数：

$$
\ln p(\mathcal{D} | \mu) =\sum_{n=1}^{N} \ln p(x_{n}|\mu) = \sum_{n=1}^{N} x_{n}\ln\mu \,+\, (1-x_{n}) \ln (1-\mu)
$$

将该函数对 $\mu$ 求导可求出最大似然估计：

$$
\mu_{ML} = \frac{1}{N} \sum_{n=1}^{N} x_{n}
$$

将测试时 $x=1$ 的次数设为 $m$ ，则原式可以化为：

$$
\mu_{ML} = \frac{m}{N}
$$

对于 $m$ 在 $N$ 中的分布，我们能给出二项分布公式：

$$
Bin(m|\mu ,N) = \binom{N}{m} \mu^{m}(1-\mu)^{N-m}
$$

其中

$$
\binom{N}{m} = \frac{N!}{(N-m)!m!}
$$

其均值和方差由下式给出：

$$
\begin{aligned}
E[m] &= \sum_{m=0}^{N}mBin(m|\mu,N) = \mu N
\\
var[m] &= \sum_{m=0}^{N}(m-E[m])^{2}Bin(m|\mu,N) = N\mu(1-\mu) 
\end{aligned}
$$

都是基础知识，不详细证明

### $\beta$ 分布

首先我们在二项分布的基础上引入贝叶斯视角，在贝叶斯统计中，我们不把 $p$ 当成固定的未知数，而是把它看作一个随机变量，拥有自己的概率分布。
这个分布就是“先验分布”。当我们通过试验（比如二项分布的试验）收集到数据后，可以更新这个分布，得到“后验分布”。而 $\beta$ 分布恰好是描述 $p$ 的自然选择。

#### 基本形式

$$
Beta(\mu|a,b) = \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\mu^{a-1}(1-\mu)^{b-1}
$$

其中

- $\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}$ 是归一化常数，使得该分布的积分为1。
- $a$ 和 $b$ 是超参数，控制概率分布函数的形状，也控制着 $\mu$ 。

它的灵活性很强：通过调整 $a$ 和 $b$ ，可以描述从均匀分布到单峰分布，甚至是 U 形分布。

#### $a$ 和 $b$ 对分布的影响

1. **均值和方差**

$$
\begin{aligned}
\mu &= \frac{a}{a+b}
\\
\sigma^{2} &= \frac{ab}{(a+b)^{2}(a+b+1)}
\end{aligned}
$$

均值随着 $a$ 相对 $b$ 的比例增大而向 1 靠拢，反之向 0 靠拢。方差则在 $a$ 和 $b$ 都较小时较大，随着两者之和增加而减小。

2. **形状特性**

- 当 $a = b$ 时，分布关于 $x = 0.5$ 对称。
  - $a = b = 1$：均匀分布（平坦）。
  - $a = b > 1$：分布呈钟形，峰值在 0.5，$a, b$ 越大越尖锐。
  - $a = b < 1$：U 形分布，两端高中间低。
- 当 $a < b$ 时，分布偏向左侧（峰值靠近 0）。
- 当 $a > b$ 时，分布偏向右侧（峰值靠近 1）。
- 当 $a < 1, b < 1$ 时，分布在 0 和 1 附近有尖峰，形成 U 形。
- 当 $a > 1, b > 1$ 时，分布更集中在中间，呈现单峰。

3. **边界行为**

- 若 $a < 1$，则 $x = 0$ 处趋向无穷（密度激增）。
- 若 $b < 1$，则 $x = 1$ 处趋向无穷。
- 若 $a > 1$，则 $x = 0$ 处密度为 0；若 $b > 1$，则 $x = 1$ 处密度为 0。

4. **特殊情况**

- $a = 1, b = 1$：均匀分布。
- $a = 1, b > 1$：密度从 0 到 1 单调递减。
- $a > 1, b = 1$：密度从 0 到 1 单调递增。
- $a, b \to \infty$（且 $a/b$ 保持常数）：分布趋向正态分布（集中于均值附近）。

5. **直观理解**

- $a$ 可以看作“正向事件”的强度，$b$ 是“反向事件”的强度。比如在贝叶斯统计中，Beta 分布常用于描述成功概率的先验分布，$a$ 和 $b$ 分别对应成功和失败的“伪计数”。
- $a + b$ 越大，分布越集中，表示“信息量”越多；$a + b$ 越小，分布越分散，表示不确定性更高。

总结来说，$a$ 和 $b$ 共同决定了 Beta 分布的对称性、偏态、峰值位置以及边界行为。通过调整这两个参数，可以灵活地塑造分布的形态以适应不同场景。

#### 从二项分布到 $\beta$ 分布的桥梁

假设我们用二项分布做实验，进行了 $n$ 次试验，成功了 $k$ 次。现在我们想用贝叶斯方法推断 $p$ 的分布：

- 先验：假设 $p$ 服从 $\beta$ 分布，比如 $Beta(a,b)$ 这是我们的初始猜测。
- 似然：实验数据服从二项分布，似然函数是 $P(k|n,p) \propto p^{k} (1-p)^{n-k}$
- 后验：根据贝叶斯定理，后验分布正比于先验乘以似然，即 $P(p|k,n) \propto P(p)P(k|n,p)$ 。代入 $\beta$ 分布和二项分布的表达式， $P(p|k,n) \propto p^{a-1}(1-p)^{b-1} p^{k}(1-p)^{n-k}$ ，合并同类项可得：

$$
P(p|k,n) \propto p^{a+k-1} (1-p)^{b+n-k-1} = Beta(a+k,b+n-k)
$$

所以后验分布仍然是一个 $\beta$ 分布，参数从 $(a,b)$ 更新为 $(a+k,b+n-k)$ ，其实从抛硬币的角度来看，就是 $a \to a+head$ 和 $b \to b+tail$

#### 逻辑的直观解释

- $a$ 和 $b$ 的含义：你可以把 $a$ 看作“虚拟的成功次数”，$b$ 看作“虚拟的失败次数”。初始的 $\beta$ 分布 $Beta(a,b)$ 是基于这些“虚拟数据”的猜测。
- 试验的贡献：二项分布提供了真实的试验数据（ $k$ 次成功， $n−k$ 次失败）。这些数据被加到先验的“虚拟计数”上，更新了我们的信念。
- 结果：后验分布 $Beta(a+k,b+n−k)$ 是结合了先验信息和观测数据的自然延续。

#### 为什么这很优雅？

- 共轭性：二项分布和 Beta 分布的数学形式天然匹配，更新后还是 $\beta$ 分布，不需要复杂的数值计算。
- 连续性：二项分布是离散的，描述具体的成功次数；$\beta$ 分布是连续的，描述 $p$ 的不确定性。它们通过贝叶斯更新连接起来。
- 直觉：这就像你在掷硬币时，先凭感觉猜正面的概率，然后根据实际掷的结果调整猜测，最后得到一个更靠谱的概率分布。

## 多项式变量

假设 $\mathbf{x} \in \left\{ 0,1,\dots,N \right\}$

$$
p(\mathbf{x | \mu}) = \prod_{k=1}^{K}(\mu_{k})^{x_{k}}
$$

### 狄利克雷分布

## 高斯分布

### 条件高斯分布

### 边际高斯分布

### 高斯变量的贝叶斯定理

### 高斯函数的最大似然

### 序贯估计

### 高斯函数的贝叶斯推理

### 周期变量

### 高斯函数的混合函数

## 指数家族

### 最大似然和充分数据

### 共轭先验

### 非信息性先验

## 非参数方法

### 核密度估计器

### 最近邻方法
